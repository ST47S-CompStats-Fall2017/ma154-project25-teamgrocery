---
title: "Forecasting Grocery Sales"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# IMPORTANT NOTES

```{r}

# 1. Set to TRUE to import all relevant CSV files. Only need to do this once
LOAD_CSV_DATA <- TRUE

# 2. Set this value to TRUE to install all of the relevant packages. Only need to do this once
DO_INSTALL_PACKAGES <- TRUE

# 3. Set TRUE to use small sample. Set to FALSE to use more data, which may take hours  
SMALLER_SAMPLE <- TRUE

# 4. Create a directory named "plots" on the same level as this .Rmd file. This code assumes the existence of that directory, saves all charts there

```

TABLE OF CONTENTS:

SECTIONS:

A -- Setup:

# 1. Install packages
# 2. Load data from CSV

B -- Main Code:

# 1. Joining tables
# 2. Sample training data, merge into one big table
# 3. Add variables to table
# 4. Split big table into separate rf, arima, and test tables
# 5. Train ARIMA model
# 6. Add ARIMA predictions to rf and test tables
# 7. Train RF models
# 8. Predict test data

C -- Data Visualization:

# 1. Different variables' influence on sales
# 2. Variable importance for two random forests
# 3. Plotting errors in sample test

# Section A -- Setup

# 1. Install packages
```{r}

if (DO_INSTALL_PACKAGES) {
  install.packages("data.table")
  install.packages('ggplot2')
  install.packages('ggthemes')
  install.packages('scales')
  install.packages('dplyr')
  install.packages('mice')
  install.packages('class')
  install.packages('lubridate')
  install.packages('caret')
  install.packages('tseries')
  install.packages("forecast")
  install.packages('tidyr')
}

library('data.table')
library('ggplot2')
library('ggthemes')
library('scales')
library('dplyr')
library('mice')
library('class')
library('lubridate')
library('caret')
library('tseries')
library('forecast')
library('tidyr')

```

# 2. Load data from CSV
```{r}

if (LOAD_CSV_DATA) {
  train <- fread("train.csv")
  holidays <- fread("holidays_events.csv")
  items <- fread("items.csv")
  oil <- fread("oil.csv")
  stores <- fread("stores.csv")
  test <- fread("test.csv")
}

```


# Section B -- Main code

# 1. Joining tables
# 2. Sample training data, merge into one big table
# 3. Add variables to table
# 4. Split big table into separate rf, arima, and test tables
# 5. Train ARIMA model
# 6. Add ARIMA predictions to rf and test tables
# 7. Train RF models
# 8. Predict test data

# 1. Joining tables
```{r}

# Set names
cols <- names(holidays)[-1] # Except for date
setnames(holidays, (cols), paste("holiday_", (cols), sep=""))

cols <- names(items)[-1] # Except for item_nbr
setnames(items, (cols), paste("item_", (cols), sep=""))

setnames(oil, "dcoilwtico", "oil_price")
setkey(oil, date)

# Remove NA values from oil by using last recorded price
set(oil, 1L, "oil_price", oil[2,oil_price]) # First row was NA, so use value of 2nd row
oil_price_na <- is.na(oil$oil_price)
last_oil_price <- oil[1,oil_price]
for (i in 1:length(oil_price_na)) {
  if (oil_price_na[i]) {
    set(oil, i, "oil_price", last_oil_price)
  } else {
    last_oil_price <- oil[i,oil_price]
  }
}

# Add missing dates.

get_oil_row <- function(y, m, d, price) {
  m <- formatC(m, width = 2, flag = "0")
  d <- formatC(d, width = 2, flag = "0")
  return(list(paste(y,m,d,sep="-"), price))
}

for (i in 1:length(oil_price_na)) {
  cur_date <- oil[i, date]
  cur_year <- as.integer(substr(cur_date,0,4))
  cur_month <- as.integer(substr(cur_date,6,7))
  cur_day <- as.integer(substr(cur_date,9,10))
  cur_oil_price <- oil[i, oil_price]
  
  # Don't check the first row
  if (i != 1) {
    
    # Add days last month and to the start of this month
    if (cur_month != last_month) {
      
      # Last month
      if (31 >= last_day + 1) {
        for (d in (last_day + 1):31) {
          oil <- rbind(oil, get_oil_row(last_year, last_month, d, last_oil_price))
        }
      }
      
      # Start of this month
      if (cur_day - 1 >= 1) {
        for (d in 1:(cur_day - 1)) {
          oil <- rbind(oil, get_oil_row(cur_year, cur_month, d, last_oil_price))
        }
      }
    }
    
    # For all skipped days in the same month
    else if (cur_day - 1 >= last_day + 1) {
      for (d in (last_day + 1):(cur_day - 1)) {
        oil <- rbind(oil, get_oil_row(cur_year, cur_month, d, last_oil_price))
      }
    }
    
  }
  
  last_year <- cur_year
  last_month <- cur_month
  last_day <- cur_day
  last_oil_price <- cur_oil_price
}


cols <- names(stores)[-1] # Except for store_nbr
setnames(stores, (cols), paste("store_", (cols), sep=""))

# Join with items
setkey(train, item_nbr)
setkey(items, item_nbr)
train <- merge(train, items, all.x = TRUE)

# Join with oil
setkey(train, date)
setkey(oil, date)
train <- merge(train, oil, all.x = TRUE)

# Join with stores
setkey(train, store_nbr)
setkey(stores, store_nbr)
train <- merge(train, stores, all.x = TRUE)


# Same, for test data

# Join with items
setkey(test, item_nbr)
setkey(items, item_nbr)
test <- merge(test, items, all.x = TRUE)

# Join with oil
setkey(test, date)
setkey(oil, date)
test <- merge(test, oil, all.x = TRUE)

# Join with stores
setkey(test, store_nbr)
setkey(stores, store_nbr)
test <- merge(test, stores, all.x = TRUE)

```

# 2. Sample training data
```{r}

# RF dataset: sample from last 15 days
set.seed(888)
if (SMALLER_SAMPLE) {
  p <- 50000/(125497040 - 123926072) # Proportion we want to sample (total, num. before Aug. 1 2016)
} else {
  p <- 200000/(125497040 - 123926072) # Proportion we want to sample (total, num. before Aug. 1 2016)
}
setkey(train, date)
ndt <- train[, .(n = .N), by = date]
indices <- c()
start <- end <- 1 + 123926072
num_days <- nrow(ndt)
for (i in (num_days - 14):num_days) {
  n <- ndt[i,n]
  end <- start + n - 1
  new_indices <- sample(start:end, as.integer(n * p))
  new_indices
  indices <- c(indices, new_indices)
  start <- end + 1
}
sandbox_rf <- train[indices,]

# ARIMA Reduced data set to work with (whole time period)
if (SMALLER_SAMPLE) {
  p <- 0.05
} else {
  p <- 0.15 # Use a lot of training data for ARIMA
}
setkey(train, date)
ndt <- train[, .(n = .N), by = date]
indices <- c()
start <- end <- 1
num_days <- nrow(ndt)
for (i in 1:num_days) {
  n <- ndt[i,n]
  end <- start + n - 1
  new_indices <- sample(start:end, as.integer(n * p))
  new_indices
  indices <- c(indices, new_indices)
  start <- end + 1
}
sandbox_arima <- train[indices,]

# Class var: 0 - RF, 1 - ARIMA, 2 - TEST - to keep track of this split we made
setDT(sandbox_rf)
setDT(sandbox_arima)
setDT(test)
sandbox_rf[,class := 0]
sandbox_arima[,class := 1]
test[,class := 2]

sandbox <- rbind(sandbox_rf, sandbox_arima)
sandbox <- rbind(sandbox, test, fill=TRUE)

num_rf <- nrow(sandbox_rf)
num_arima <- nrow(sandbox_arima)

```

# 3. Add variables
```{r}

# Add cols for "year", "month", "weekday", "days_since_payday"

base <- c(2,5,5,1,3,6,1,4,0,2,5,0, 
          3,6,6,2,4,0,2,5,1,3,6,1, 
          4,0,0,3,5,1,3,6,2,4,0,2, 
          5,1,2,5,0,3,5,1,4,6,2,4,
          0,3,3,6,1,4,6,2)
sandbox[,year := as.integer(substr(date,0,4))]
sandbox[,month := as.integer(substr(date,6,7))]
sandbox[,weekday := as.integer(substr(date,9,10))]
sandbox[,days_since_payday := ifelse(weekday < 15, weekday, weekday - 15)]
sandbox[,weekday := ((base)[(year - 2013L)*12 + month] + weekday) %% 7]

# Make a categorical variable for each weekday
weekday_names <- c("Sun", "Mon", "Tues", "Weds", "Thurs", "Fri", "Sat")
for (i in 1:7) {
  weekday_name <- weekday_names[i]
  sandbox[,(weekday_name) := weekday == (i)]
}

# Make a categorical variable for each item family 
item_families <- unique(sandbox$item_family)
for (i in 1:length(item_families)) {
  cur_item_family <- item_families[i]
  sandbox[,(cur_item_family) := item_family == (cur_item_family)]
}

# Make a cat. var. for each store city
store_cities <- unique(sandbox$store_city)
for (i in 1:length(store_cities)) {
  cur_store_city <- store_cities[i]
  sandbox[,(cur_store_city) := store_city == (cur_store_city)]
}

# Make a binary variable for each store cluster
store_clusters <- unique(sandbox$store_cluster)
for (i in 1:length(store_clusters)) {
  cur_store_cluster <- store_clusters[i]
  sandbox[,(paste("Cluster", cur_store_cluster)) := store_cluster == (cur_store_cluster)]
}

# Make a binary variable for each store type
store_types <- unique(sandbox$store_type)
for (i in 1:length(store_types)) {
  cur_store_type <- store_types[i]
  sandbox[,(paste("Store Type", cur_store_type)) := store_type == (cur_store_type)]
}

# Adding months and days since Jan. 1 2013 (both starting at 1)
sandbox[,month_count := 12 * (year - 2013) + month]

```

# 4 Splitting into separate rf, arima, and test tables
```{r}

# Make data table for which items are sold every day
tmp <- train[date == "2017-08-01"]
cut <- min(tmp$id)

# Choose those sold every day
setkey(train, id)
t_pop <- train[id >= (cut), j = .(n = .N), by = .(store_nbr, item_nbr)]
t_pop[,popular := n == 15]

# Split sandbox accordingly
setDT(t_pop)
setkey(sandbox, store_nbr, item_nbr)
setkey(t_pop, store_nbr, item_nbr)
s_all <- merge(sandbox, t_pop, all.x = TRUE)

# For NA values, assume popular is false
s_all[,popular := ifelse(is.na(popular),FALSE,popular)]

# Splitting back up by class
sandbox_rf2 <- s_all[class == 0]
sandbox_rf2[,class:=NULL]
arima2 <- s_all[class == 1]
arima2[,class:=NULL]
test2 <- s_all[class == 2]
test2[,class:=NULL]

rf_popular <- sandbox_rf2[popular == TRUE]
rf_not_popular <- sandbox_rf2[popular == FALSE]
arima_popular <- arima2[popular == TRUE]
test_popular <- test2[popular == TRUE]
test_not_popular <- test2[popular == FALSE]

```

# 5. Train ARIMA Model
```{r}

family_data <- data.frame()
daily_sales <- arima_popular %>% group_by(date,item_family) %>% summarise(daily_class_sales = mean(unit_sales)) %>% filter(date > as.Date('2016-1-1')) #take the data from 2016 onwards 

for (i in unique(daily_sales$item_family)){
  daily_sales_ts <- daily_sales %>% filter(item_family == i) #filter by the class
  ds_ts <- ts(daily_sales_ts$daily_class_sales, frequency = 7) #create the time series with a week period
  ds_ts <- tsclean(ds_ts) #remove outliers
  ds_ts_arima <- auto.arima(ds_ts) #take out stepwise and 
  #approximation to make it go faster
  forecast <- forecast(ds_ts_arima, h= 14) #forecast 14 days ahead
  forecast_values <- forecast$mean #obtain the values for the forecast
  family_data <- rbind(family_data, forecast_values) #add these to the dataframe
}

family_data <- cbind(unique(daily_sales$item_family), family_data) #add family names to data frame
colnames(family_data) <- c("Family",1:14)

tidy_family_data <- gather(family_data,"Day","Sales",2:15) #tidy up
tidy_family_data$Day <- as.numeric(tidy_family_data$Day)

```

# 6. Add ARIMA results to other rf and test tables
```{r}

# Clean up to get cols "date", "item_family", "popularity"

train_pop_data <- daily_sales
setDT(train_pop_data)
setnames(train_pop_data, "daily_class_sales", "popularity")

test_pop_data <- tidy_family_data
setDT(test_pop_data)
setnames(test_pop_data, c("Family", "Day", "Sales"), c("item_family","date","popularity"))
test_dates <- unique(test$date)
test_pop_data[,date := (test_dates)[date]]

# Join with rf_popular and test_popular

setkey(rf_popular, date, item_family)
setkey(train_pop_data, date, item_family)
rf_popular <- merge(rf_popular, train_pop_data, all.x = TRUE)

setkey(test_popular, date, item_family)
setkey(test_pop_data, date, item_family)
test_popular <- merge(test_popular, test_pop_data, all.x = TRUE)

# Change NA values to 0
rf_popular[,popularity := ifelse(is.na(popularity),0,popularity)]
test_popular[,popularity := ifelse(is.na(popularity),0,popularity)]

```

# 7. Train random forests
```{r}

# Train/test data for popular items
set.seed(888)
s_p_indices <- sample(1:nrow(rf_popular), 0.7*nrow(rf_popular))
s_p_train <- rf_popular[s_p_indices]
s_p_test <- rf_popular[-s_p_indices]

# Train/test data for unpopular items
set.seed(888)
s_np_indices <- sample(1:nrow(rf_not_popular), 0.7*nrow(rf_not_popular))
s_np_train <- rf_not_popular[s_np_indices]
s_np_test <- rf_not_popular[-s_np_indices]

# Variables to use for the random forest
s_p_cols <- s_np_cols <- c("unit_sales", item_families, paste("Cluster",store_clusters), store_cities, paste("Store Type", store_types), weekday_names, "days_since_payday", "item_perishable", "oil_price")
s_p_cols <- c(s_p_cols, "popularity")

# Model for popular items
p_model_data <- s_p_train[,s_p_cols, with=FALSE]
set.seed(888)
p_model <- train(unit_sales ~ . , data=p_model_data, method="rf",
                  trControl = trainControl(method="oob"),  # Use the OOB error rate 
                  ntree=500, tuneGrid = data.frame(mtry=((length(s_p_cols) - 1))/3),
                  importance = TRUE)  # need this for variable importance below
p_predictions <- predict(p_model, newdata = s_p_test)
p_predictions <- ifelse(p_predictions < 0, 0, p_predictions)
p_actual <- s_p_test$unit_sales
p_actual <- ifelse(p_actual < 0, 0, p_actual)
p_accuracy <- sum((log(p_predictions + 1) - log(p_actual + 1)) ** 2) / length(p_predictions)
p_accuracy

# Model for unpopular items
np_model_data <- s_np_train[,s_np_cols, with=FALSE]
set.seed(888)
np_model <- train(unit_sales ~ . , data=np_model_data, method="rf",
                  trControl = trainControl(method="oob"),  # Use the OOB error rate 
                  ntree=500, tuneGrid = data.frame(mtry=((length(s_np_cols) - 1))/3),
                  importance = TRUE)  # need this for variable importance below
np_predictions <- predict(np_model, newdata = s_np_test)
np_predictions <- ifelse(np_predictions < 0, 0, np_predictions)
np_actual <- s_np_test$unit_sales
np_actual <- ifelse(np_actual < 0, 0, np_actual)
np_accuracy <- sum((log(np_predictions + 1) - log(np_actual + 1)) ** 2) / length(np_predictions)
np_accuracy

# Importance
p_model_imp <- varImp(p_model)$importance
np_model_imp <- varImp(np_model)$importance

```

# 8. Predict test data
```{r}

# Predict test_popular and test_not_popular
test_p_predictions <- predict(p_model, newdata = test_popular)
test_p_predictions <- ifelse(test_p_predictions < 0, 0, test_p_predictions)
test_np_predictions <- predict(np_model, newdata = test_not_popular)
test_np_predictions <- ifelse(test_np_predictions < 0, 0, test_np_predictions)

# Output to csv
final_predictions <- data.table(id = c(test_popular$id, test_not_popular$id), unit_sales = c(test_p_predictions, test_np_predictions))
fwrite(final_predictions, "final_predictions.csv")

```


# Section C -- Data visualization

# 1. Store popularity
# 2. Arima model
# 3. Different variables' influence on sales
# 4. Variable importance for two random forests
# 5. Plotting errors in sample test
# 6. Affect of popularity on unit sales

# 1. Store popularity
```{r}

ggplot(data = t_pop) + geom_histogram(aes(x = n, y =..density..)) + xlab("Number of Days Sold 8/1/16 to 8/15/16") + ggtitle("Distribution of Item Popularities")
ggsave("plots/popularity_distribution.jpg")

```

# 2. Arima model
```{r}

ggplot() + geom_line(data = tidy_family_data, aes(x= Day, y= Sales, color = Family)) #plot
ggsave("plots/arima_forecasts.jpg")

```

# 3. Different variables' influence on sales
```{r}

# How item_family affects sales
item_family_sales <- rf_popular[,j = .(mean_sales = mean(unit_sales)), by = item_family]
ggplot(data = item_family_sales) + geom_col(aes(x = item_family, y = mean_sales)) +
  expand_limits(y = 0) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  xlab("Item Family") + ylab("Mean Unit Sales") + ggtitle("Effect of Item Family for Highly-Transacted Items")
ggsave("plots/1p.jpg")

# How store_city affects sales
store_city_sales <- rf_popular[,j = .(mean_sales = mean(unit_sales)), by = store_city]
ggplot(data = store_city_sales) + geom_col(aes(x = store_city, y = mean_sales)) +
  expand_limits(y = 0) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("Store City") + ylab("Mean Unit Sales") + ggtitle("Effect of Store City for Highly-Transacted Items")
ggsave("plots/2p.jpg")

# How store_cluster affects sales
store_cluster_sales <- rf_popular[,j = .(mean_sales = mean(unit_sales)), by = store_cluster]
ggplot(data = store_cluster_sales) + geom_col(aes(x = store_cluster, y = mean_sales)) +
  expand_limits(y = 0) + 
  xlab("Sore Cluster") + ylab("Mean Unit Sales") + ggtitle("Effect of Store Cluster for Highly-Transacted Items")
ggsave("plots/3p.jpg")

# How store_type affects sales
store_type_sales <- rf_popular[,j = .(mean_sales = mean(unit_sales)), by = store_type]
ggplot(data = store_type_sales) + geom_col(aes(x = store_type, y = mean_sales)) +
  expand_limits(y = 0) + 
  xlab("Store Type") + ylab("Mean Unit Sales") + ggtitle("Effect of Store Type for Highly-Transacted Items")
ggsave("plots/4p.jpg")

# How day of week affects sales
weekday_sales <- rf_popular[,j = .(mean_sales = mean(unit_sales)), by = weekday]
weekday_sales[,weekday := (weekday_names)[weekday + 1]]
weekday_sales$weekday <- factor(weekday_sales$weekday, levels = (weekday_names))
ggplot(data = weekday_sales) + geom_col(aes(x = weekday, y = mean_sales)) +
  expand_limits(y = 0) +
  xlab("Day of Week") + ylab("Mean Unit Sales") + ggtitle("Effect of Day of Week for Highly-Transacted Items")
ggsave("plots/5p.jpg")

# How days since payday affects sales
days_since_payday_sales <- rf_popular[,j = .(mean_sales = mean(unit_sales)), by = days_since_payday]
ggplot(data = days_since_payday_sales) + geom_col(aes(x = days_since_payday, y = mean_sales)) +
  expand_limits(y = 0) + 
  xlab("Days Since Payday") + ylab("Mean Unit Sales") + ggtitle("Effect of Days Since Payday for Highly-Transacted Items")
ggsave("plots/6p.jpg")

# How oil price affects sales
oil_price_sales <- rf_popular[,j = .(mean_sales = mean(unit_sales)), by = oil_price]
ggplot(data = oil_price_sales) + geom_smooth(aes(x = oil_price, y = mean_sales)) +
  expand_limits(y = 0) + 
  xlab("Oil Price") + ylab("Mean Unit Sales") + ggtitle("Effect of Oil Price for Highly-Transacted Items")
ggsave("plots/7p.jpg")

# How item perishibality affects sales
item_perishable_sales <- rf_popular[,j = .(mean_sales = mean(unit_sales)), by = item_perishable]
ggplot(data = item_perishable_sales) + geom_col(aes(x = item_perishable, y = mean_sales)) +
  expand_limits(y = 0) + 
  xlab("Item Perishable") + ylab("Mean Unit Sales") + ggtitle("Effect of Item Perishability for Highly-Transacted Items")
ggsave("plots/8p.jpg")


## Same code, but for non-highly-transacted items

# How item_family affects sales
item_family_sales <- rf_not_popular[,j = .(mean_sales = mean(unit_sales)), by = item_family]
ggplot(data = item_family_sales) + geom_col(aes(x = item_family, y = mean_sales)) +
  expand_limits(y = 0) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  xlab("Item Family") + ylab("Mean Unit Sales") + ggtitle("Effect of Item Family for Non-Highly-Transacted Items")
ggsave("plots/1np.jpg")

# How store_city affects sales
store_city_sales <- rf_not_popular[,j = .(mean_sales = mean(unit_sales)), by = store_city]
ggplot(data = store_city_sales) + geom_col(aes(x = store_city, y = mean_sales)) +
  expand_limits(y = 0) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("Store City") + ylab("Mean Unit Sales") + ggtitle("Effect of Store City for Non-Highly-Transacted Items")
ggsave("plots/2np.jpg")

# How store_cluster affects sales
store_cluster_sales <- rf_not_popular[,j = .(mean_sales = mean(unit_sales)), by = store_cluster]
ggplot(data = store_cluster_sales) + geom_col(aes(x = store_cluster, y = mean_sales)) +
  expand_limits(y = 0) + 
  xlab("Sore Cluster") + ylab("Mean Unit Sales") + ggtitle("Effect of Store Cluster for Non-Highly-Transacted Items")
ggsave("plots/3np.jpg")

# How store_type affects sales
store_type_sales <- rf_not_popular[,j = .(mean_sales = mean(unit_sales)), by = store_type]
ggplot(data = store_type_sales) + geom_col(aes(x = store_type, y = mean_sales)) +
  expand_limits(y = 0) + 
  xlab("Store Type") + ylab("Mean Unit Sales") + ggtitle("Effect of Store Type for Non-Highly-Transacted Items")
ggsave("plots/4np.jpg")

# How day of week affects sales
weekday_sales <- rf_not_popular[,j = .(mean_sales = mean(unit_sales)), by = weekday]
weekday_sales[,weekday := (weekday_names)[weekday + 1]]
weekday_sales$weekday <- factor(weekday_sales$weekday, levels = (weekday_names))
ggplot(data = weekday_sales) + geom_col(aes(x = weekday, y = mean_sales)) +
  expand_limits(y = 0) +
  xlab("Day of Week") + ylab("Mean Unit Sales") + ggtitle("Effect of Day of Week for Non-Highly-Transacted Items")
ggsave("plots/5np.jpg")

# How days since payday affects sales
days_since_payday_sales <- rf_not_popular[,j = .(mean_sales = mean(unit_sales)), by = days_since_payday]
ggplot(data = days_since_payday_sales) + geom_col(aes(x = days_since_payday, y = mean_sales)) +
  expand_limits(y = 0) + 
  xlab("Days Since Payday") + ylab("Mean Unit Sales") + ggtitle("Effect of Days Since Payday for Non-Highly-Transacted Items")
ggsave("plots/6np.jpg")

# How oil price affects sales
oil_price_sales <- rf_not_popular[,j = .(mean_sales = mean(unit_sales)), by = oil_price]
ggplot(data = oil_price_sales) + geom_smooth(aes(x = oil_price, y = mean_sales)) +
  expand_limits(y = 0) + 
  xlab("Oil Price") + ylab("Mean Unit Sales") + ggtitle("Effect of Oil Price for Non-Highly-Transacted Items")
ggsave("plots/7np.jpg")

# How item perishibality affects sales
item_perishable_sales <- rf_not_popular[,j = .(mean_sales = mean(unit_sales)), by = item_perishable]
ggplot(data = item_perishable_sales) + geom_col(aes(x = item_perishable, y = mean_sales)) +
  expand_limits(y = 0) + 
  xlab("Item Perishable") + ylab("Mean Unit Sales") + ggtitle("Effect of Item Perishability for Non-Highly-Transacted Items")
ggsave("plots/8np.jpg")

```

# 4. Variable importance for two random forests
```{r, fig.width=3,fig.height=7}

ggplot(varImp(p_model, n = 10))
ggsave("plots/varimp_p.jpg")
ggplot(varImp(np_model, n = 10))
ggsave("plots/varimp_np.jpg")

```

# 5. Plotting errors in sample test
```{r}

p_err <- p_predictions - p_actual
p_err_prop <- p_err/p_actual
np_err <- np_predictions - np_actual
np_err_prop <- np_err/np_actual

ggplot(data = data.table(error = p_err)) + geom_density(aes(x = error)) + 
  xlim(-50, 50) + xlab("Prediction Minus Actual") + 
  ggtitle("Random Forest Prediction Errors for Highly-Transacted Items")
ggsave("plots/err_p.jpg")

ggplot(data = data.table(error = p_actual)) + geom_density(aes(x = error)) + 
  xlim(0, 50) + xlab("Unit Sales") + 
  ggtitle("Unit Sales for Highly-Transacted Items")
ggsave("plots/actual_p.jpg")

ggplot(data = data.table(error = p_err_prop)) + geom_density(aes(x = error)) +
  xlim(-2, 15) + xlab("Scaled Error") + 
  ggtitle("Scaled Random Forest Prediction Errors for Highly-Transacted Items")
ggsave("plots/err_scaled_p.jpg")

ggplot(data = data.table(error = np_err)) + geom_density(aes(x = error)) + 
  xlim(-20, 20) + xlab("Prediction Minus Actual") + 
  ggtitle("Random Forest Prediction Errors for Non-Highly-Transacted Items")
ggsave("plots/err_np.jpg")

ggplot(data = data.table(error = np_actual)) + geom_histogram(aes(x = error), binwidth = 1) + 
  xlim(0, 20) + xlab("Unit Sales") + 
  ggtitle("Unit Sales for Non-Highly-Transacted Items")
ggsave("plots/actual_np.jpg")

ggplot(data = data.table(error = np_err_prop)) + geom_density(aes(x = error)) +
  xlim(-2, 15) + xlab("Scaled Error") + 
  ggtitle("Scaled Random Forest Prediction Errors for Non-Highly-Transacted Items")
ggsave("plots/err_scaled_np.jpg")

```

# 6. Affect of popularity on unit sales
```{r}

ggplot(data = s_p_test) + geom_smooth(aes(x = popularity, y = unit_sales)) +
  expand_limits(y = 0) + 
  xlab("Popularity") + ylab("Unit Sales") + ggtitle("Correlation of Popularity and Unit Sales")
ggsave("plots/popularity_sales.jpg")

```
