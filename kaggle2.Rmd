---
title: "testdata"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Install packages
```{r}

# Load data: only need to run once

#install.packages("data.table")
#library(data.table)
#ships <- fread('https://raw.githubusercontent.com/hopperrr/ellis-immigration-by-ship/gh-pages/data/trips.tsv')

# Total number of people from each ship

library('ggplot2')
#install.packages('ggthemes')
library('ggthemes')
library('scales')
library('dplyr')
#install.packages("mice")
library('mice')
#install.packages("randomForest")
library('randomForest')
#install.packages("class")
library('class')
#install.packages("data.table")
library('data.table')
library('lubridate')
library('caret')

#install.packages('tseries')
library(tseries)


```

# Load data from CSV
```{r}

# This data import takes a while
#train <- fread("train.csv")

# This date import is quick
holidays <- fread("holidays_events.csv")
items <- fread("items.csv")
oil <- fread("oil.csv")
stores <- fread("stores.csv")

```

# SECTIONS
# 1. Joining tables
# 2. Sample a sandbox
# 3. Add variables
# 4. Split into transacted/non-transacted
# 5. ARIMA for highly transacted items
# 6. RF models

# Extra stuff
# A. Data visualization

# 1. Joining tables
```{r}

# Set names
cols <- names(holidays)[-1] # Except for date
setnames(holidays, (cols), paste("holiday_", (cols), sep=""))

cols <- names(items)[-1] # Except for item_nbr
setnames(items, (cols), paste("item_", (cols), sep=""))

setnames(oil, "dcoilwtico", "oil_price")

cols <- names(stores)[-1] # Except for store_nbr
setnames(stores, (cols), paste("store_", (cols), sep=""))

# Join with holidays_events
#setkey(train, date)
#setkey(holidays, date)
#train <- merge(train, holidays, all.x = TRUE)

# Join with items
setkey(train, item_nbr)
setkey(items, item_nbr)
train <- merge(train, items, all.x = TRUE)

# Join with oil
setkey(train, date)
setkey(oil, date)
train <- merge(train, oil, all.x = TRUE)

# Join with stores
setkey(train, store_nbr)
setkey(stores, store_nbr)
train <- merge(train, stores, all.x = TRUE)

```

# 2. Sample a sandbox
```{r}

# Reduced data set to work with
set.seed(888)
p <- 15000/125497040 # Proportion we want to sample
setkey(train, date)
ndt <- train[, .(n = .N), by = date]
indices <- c()
start <- end <- 1
num_days <- nrow(ndt)
for (i in 1:num_days) {
  n <- ndt[i,n]
  end <- start + n - 1
  new_indices <- sample(start:end, as.integer(n * p))
  new_indices
  indices <- c(indices, new_indices)
  start <- end + 1
}
sandbox <- train[indices,]

```

# 3. Add variables
```{r}

# Add cols for "year", "month", "day" (values 1-365)
sandbox[,c("year", "month", "day", "weekday") := 0]

dates_str <- sandbox$date
dates <- lapply(strsplit(dates_str, "-"), as.integer)
for (i in 1:length(dates)) {
  # Calculate vals
  date = dates[[i]]
  y = date[1]
  m = date[2]
  d = yday(ymd(dates_str[i])) # Day of year
  w = wday(ymd(dates_str[i])) # Day of week
  
  # Update table
  set(sandbox, i, "year", y)
  set(sandbox, i, "month", m)
  set(sandbox, i, "day", d)
  if (d < 15) {
    set(sandbox, i, "days_since_payday", d)  
  } else {
    set(sandbox, i, "days_since_payday", d - 15)
  }
  set(sandbox, i, "weekday", w)
}

# Make a categorical variable for each weekday
weekday_names <- c("Sun", "Mon", "Tues", "Weds", "Thurs", "Fri", "Sat")
for (i in 1:7) {
  weekday_name <- weekday_names[i]
  sandbox[,(weekday_name) := weekday == (i)]
}

# Make a categorical variable for each item family 
item_families <- unique(sandbox$item_family)
for (i in 1:length(item_families)) {
  cur_item_family <- item_families[i]
  sandbox[,(cur_item_family) := item_family == (cur_item_family)]
}

# Make a cat. var. for each store city
store_cities <- unique(sandbox$store_city)
for (i in 1:length(store_cities)) {
  cur_store_city <- store_cities[i]
  sandbox[,(cur_store_city) := store_city == (cur_store_city)]
}
# Make a binary variable for each store cluster
store_clusters <- unique(sandbox$store_cluster)
for (i in 1:length(store_clusters)) {
  cur_store_cluster <- store_clusters[i]
  sandbox[,(paste("Cluster", cur_store_cluster)) := store_cluster == (cur_store_cluster)]
}

# Make a binary variable for each store type
store_types <- unique(sandbox$store_type)
for (i in 1:length(store_types)) {
  cur_store_type <- store_types[i]
  sandbox[,(paste("Store Type", cur_store_type)) := store_type == (cur_store_type)]
}

# Adding months and days since Jan. 1 2013 (both starting at 1)
setDT(sandbox)
sandbox[,month_count := 12 * (year - 2013) + month]
sandbox[,day_count := 365 * (year - 2013) + day]

```

# 4. Split into transacted/non-transacted (Todo Thomas)

```{r}
item_ids = unique(sandbox$id)
store_ids = unique(stores$store_nbr)
item_store_sales = data.table(matrix(0, nrow=length(item_ids), ncol=length(store_ids)))
for (i in 1:length(item_ids)) {
  for (j in 1:length(store_ids)) {
    cur_item_id = item_ids[i]
    cur_store_nbr = store_ids[j]
    item_store_sales[i,j] = nrow(sandbox[sandbox$id==cur_item_id && sandbox$store_nbr==cur_store_nbr])
  }
}

# Need actual matrix of days open for every store
total_days <- max(sandbox$day_count)
store_data <- sandbox[,j = .(days_open = (total_days) - min(day_count)), by=store_nbr]
setkey(store_data, store_nbr)
days_open <- store_data$days_open

days_open = matrix(1322, nrow=1, ncol=length(store_ids))

id_frequency = data.table(item_ids, rep(0,length(train_ids)))
colnames(id_frequency) = c("id", "ratio")
for (i in 1:length(item_ids)) {
  total_transaction_days = sum(item_store_sales[i,])
  days_open_of_stores_w_item = 0
  for (j in 1:length(store_ids)) {
    if (item_store_sales[i,j] > 0) {
      days_open_of_stores_w_item = days_open[1,store_ids[j]] + days_open_of_stores_w_item
    }
  }
  id_frequency[id_frequency$item_ids==item_ids[i]] = total_transaction_days / days_open_of_stores_w_item
}

popular_items = filter(id_frequency, ratio>0.95)
popular_items_ids = popular_items$id
```

# 5. Transacted: time series (Justin)

THIS CHUNK: 
1. Set the time series variable
2. Apply ARIMA model on an Example
```{r}
#View(s_train)
library(dplyr)
library('ggplot2')
#install.packages("forecast")
library('forecast')
library('tseries')
library(data.table)

#summarize unit sales by class for the day
daily_sales <- s_train %>% group_by(date,item_family) %>% summarise(daily_class_sales = mean(unit_sales)) %>% filter(date > as.Date('2016-1-1'))
grocery_daily_sales = daily_sales %>% filter(item_family == "BEVERAGES")
grocery_daily_sales
ds_ts <- ts(grocery_daily_sales$daily_class_sales, frequency = 7)
ds_ts <- tsclean(ds_ts)
plot(ds_ts)
fit <- auto.arima(ds_ts, stepwise = FALSE, approximation = FALSE)
fore <- forecast(fit, h= 14)
plot(fore)
fore$mean
```

THIS CHUNK:
1. Generate Data Table of Arima Forecasts by Class
```{r}
library(tidyr)
family_data <- data.frame()
daily_sales <- s_train %>% group_by(date,item_family) %>% summarise(daily_class_sales = mean(unit_sales)) %>% filter(date > as.Date('2016-1-1')) #take the data from 2016 onwards 

#checks to see if there are enough obervations aka we need more data points
#we need at least enough data points for each class to have an observation each day
for (i in unique(daily_sales$item_family)){
  daily_sales_class <- daily_sales %>% filter(item_family == i)
  if (nrow(daily_sales_class)<150){
    daily_sales <- daily_sales %>% filter(item_family != i)
  }
}


for (i in unique(daily_sales$item_family)){
  daily_sales_ts <- daily_sales %>% filter(item_family == i) #filter by the class
  ds_ts <- ts(daily_sales_ts$daily_class_sales, frequency = 7) #create the time series with a week period
  ds_ts <- tsclean(ds_ts) #remove outliers
  ds_ts_arima <- auto.arima(ds_ts, stepwise = FALSE, approximation = FALSE) #take out stepwise and 
  #approximation to make it go faster
  forecast <- forecast(ds_ts_arima, h= 14) #forecast 14 days ahead
  forecast_values <- forecast$mean #obtain the values for the forecast
  family_data <- rbind(family_data, forecast_values) #add these to the dataframe
}


family_data <- cbind(unique(daily_sales$item_family), family_data) #add family names to data frame
colnames(family_data) <- c("Family",1:14)
family_data
tidy_family_data <- gather(family_data,"Day","Sales",2:15) #tidy up
tidy_family_data$Day <- as.numeric(tidy_family_data$Day)
daily_sales_ts
ggplot() + geom_line(data = tidy_family_data, aes(x= Day, y= Sales, color = Family)) #plot
```

# 6. Transacted: random forest
```{r}

# Need
# - s_popular (has popularity column)
# - s_not_popular

# Train/test data for popular items
s_p_indices <- sample(s_popular, 0.7*nrow(s_popular))
s_p_train <- s_popular[s_p_indices]
s_p_test <- s_popular[-s_p_indices]

# Train/test data for unpopular items
s_np_indices <- sample(s_not_popular, 0.7*nrow(s_not_popular))
s_np_train <- s_popular[s_np_indices]
s_np_test <- s_popular[-s_np_indices]

# Variables to use for the random forest
s_np_cols <- c("unit_sales", item_families, paste("Cluster",store_clusters), store_cities, paste("Store Type", store_types), weekday_names)
s_p_cols <- c(s_np_cols, "popularity")

# Model for popular items
p_model_data <- s_p_train[,s_p_cols, with=FALSE]
p_model <- train(unit_sales ~ . , data=p_model_data, method="rf",
                  trControl = trainControl(method="oob"),  # Use the OOB error rate 
                  ntree=500, tuneGrid = data.frame(mtry=((length(s_p_cols) - 1))/3),
                  importance = TRUE)  # need this for variable importance below
p_predictions <- predict(p_model, newdata = s_p_test)
p_actual <- s_p_test$unit_sales
p_accuracy <- sum((log(p_predictions + 1) - log(p_actual + 1)) ** 2) / length(p_predictions)
p_accuracy

# Model for unpopular items
np_model_data <- s_p_train[,s_np_cols, with=FALSE]
np_model <- train(unit_sales ~ . , data=np_model_data, method="rf",
                  trControl = trainControl(method="oob"),  # Use the OOB error rate 
                  ntree=500, tuneGrid = data.frame(mtry=((length(s_np_cols) - 1))/3),
                  importance = TRUE)  # need this for variable importance below
np_predictions <- predict(np_model, newdata = s_np_test)
np_actual <- s_np_test$unit_sales
np_accuracy <- sum((log(np_predictions + 1) - log(np_actual + 1)) ** 2) / length(np_predictions)
np_accuracy

```


# EXTRA STUFF

# A. Data visualization
```{r}

# TODO
# - separate visualization for highly vs. non-highly-transacted items
# - divide by stores_open (add variable for that)

# How store_cluster affects sales
store_cluster_sales <- sandbox[,j = .(mean_sales = mean(unit_sales)), by = store_cluster]
ggplot(data = store_cluster_sales) + geom_col(aes(x = store_cluster, y = mean_sales)) +
  expand_limits(y = 0)

# How store_city affects sales
store_city_sales <- sandbox[,j = .(mean_sales = mean(unit_sales)), by = store_city]
ggplot(data = store_city_sales) + geom_col(aes(x = store_city, y = mean_sales)) +
  expand_limits(y = 0)

# How item_perishable affects sales
item_perishable_sales <- sandbox[,j = .(mean_sales = mean(unit_sales)), by = item_perishable]
ggplot(data = item_perishable_sales) + geom_col(aes(x = item_perishable, y = mean_sales)) +
  expand_limits(y = 0)

# How store_type affects sales
store_type_sales <- sandbox[,j = .(mean_sales = mean(unit_sales)), by = store_type]
ggplot(data = store_type_sales) + geom_col(aes(x = store_type, y = mean_sales)) +
  expand_limits(y = 0)

# How weekday affects sales
weekday_sales <- sandbox[,j = .(mean_sales = mean(unit_sales)), by = weekday]
ggplot(data = weekday_sales) + geom_col(aes(x = weekday, y = mean_sales)) +
  expand_limits(y = 0)

# How onpromotion affects sales
onpromotion_sales <- sandbox[,j = .(mean_sales = mean(unit_sales)), by = onpromotion]
ggplot(data = onpromotion_sales) + geom_col(aes(x = onpromotion, y = mean_sales)) +
  expand_limits(y = 0)


```
