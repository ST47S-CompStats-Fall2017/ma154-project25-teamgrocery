---
title: "testdata"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Install packages
```{r}

# Load data: only need to run once

#install.packages("data.table")
#library(data.table)
#ships <- fread('https://raw.githubusercontent.com/hopperrr/ellis-immigration-by-ship/gh-pages/data/trips.tsv')

# Total number of people from each ship

library('ggplot2')
#install.packages('ggthemes')
library('ggthemes')
library('scales')
library('dplyr')
#install.packages("mice")
library('mice')
#install.packages("randomForest")
library('randomForest')
#install.packages("class")
library('class')
#install.packages("data.table")
library('data.table')
library('lubridate')
library('caret')

#install.packages('tseries')
library(tseries)
#install.packages("forecast")
library('forecast')


```

# Load data from CSV
```{r}

# This data import takes a while
#train <- fread("train.csv")

# This date import is quick
holidays <- fread("holidays_events.csv")
items <- fread("items.csv")
oil <- fread("oil.csv")
stores <- fread("stores.csv")

test <- fread("test.csv")

```

# SECTIONS
# 1. Joining tables
# 2. Sample a sandbox
# 3. Add variables
# 4. Split into transacted/non-transacted
# 4b. Split sandbox into transacted/non-transacted
# 5. ARIMA for highly transacted items
# 6. RF models

# Extra stuff
# A. Data visualization

# 1. Joining tables

```{r}
# Set names
cols <- names(holidays)[-1] # Except for date
setnames(holidays, (cols), paste("holiday_", (cols), sep=""))

cols <- names(items)[-1] # Except for item_nbr
setnames(items, (cols), paste("item_", (cols), sep=""))

setnames(oil, "dcoilwtico", "oil_price")

cols <- names(stores)[-1] # Except for store_nbr
setnames(stores, (cols), paste("store_", (cols), sep=""))

# Join with holidays_events
#setkey(train, date)
#setkey(holidays, date)
#train <- merge(train, holidays, all.x = TRUE)

# Join with items
setkey(test, item_nbr)
setkey(items, item_nbr)
test <- merge(test, items, all.x = TRUE)

# Join with oil
setkey(test, date)
setkey(oil, date)
test <- merge(test, oil, all.x = TRUE)

# Join with stores
setkey(test, store_nbr)
setkey(stores, store_nbr)
test <- merge(test, stores, all.x = TRUE)
```

# 2. Sample a sandbox (only in last 2 weeks)
```{r}

num_rf <- 300000 # Currently x10, now x4 from first
num_arima <- 1000000 # Was 60,000, now x4 from first

# RF Reduced data set to work with (last 15 days)
set.seed(888)
p <- num_rf/(125497040 - 123926072) # Proportion we want to sample (total, num. before Aug. 1 2016)
setkey(train, date)
ndt <- train[, .(n = .N), by = date]
indices <- c()
start <- end <- 1 + 123926072
num_days <- nrow(ndt)
for (i in (num_days - 14):num_days) {
  n <- ndt[i,n]
  end <- start + n - 1
  new_indices <- sample(start:end, as.integer(n * p))
  new_indices
  indices <- c(indices, new_indices)
  start <- end + 1
}
sandbox_rf <- train[indices,]
num_rf <- nrow(sandbox_rf)

# ARIMA Reduced data set to work with (whole time period)
p <- num_arima/125497040
setkey(train, date)
ndt <- train[, .(n = .N), by = date]
indices <- c()
start <- end <- 1
num_days <- nrow(ndt)
for (i in 1:num_days) {
  n <- ndt[i,n]
  end <- start + n - 1
  new_indices <- sample(start:end, as.integer(n * p))
  new_indices
  indices <- c(indices, new_indices)
  start <- end + 1
}
sandbox_arima <- train[indices,]
num_arima <- nrow(sandbox_arima)

sandbox <- rbind(sandbox_rf, sandbox_arima)
sandbox <- rbind(sandbox, test, fill=TRUE)

```


# 3. Add variables
```{r}

# Add cols for "year", "month", "weekday", "days_since_payday"

base <- c(2,5,5,1,3,6,1,4,0,2,5,0, 
          3,6,6,2,4,0,2,5,1,3,6,1, 
          4,0,0,3,5,1,3,6,2,4,0,2, 
          5,1,2,5,0,3,5,1,4,6,2,4,
          0,3,3,6,1,4,6,2)
sandbox[,year := as.integer(substr(date,0,4))]
sandbox[,month := as.integer(substr(date,6,7))]
sandbox[,weekday := as.integer(substr(date,9,10))]
sandbox[,days_since_payday := ifelse(weekday < 15, weekday, weekday - 15)]
sandbox[,weekday := ((base)[(year - 2013L)*12 + month] + weekday) %% 7]

# Make a categorical variable for each weekday
weekday_names <- c("Sun", "Mon", "Tues", "Weds", "Thurs", "Fri", "Sat")
for (i in 1:7) {
  weekday_name <- weekday_names[i]
  sandbox[,(weekday_name) := weekday == (i)]
}

# Make a categorical variable for each item family 
item_families <- unique(sandbox$item_family)
for (i in 1:length(item_families)) {
  cur_item_family <- item_families[i]
  sandbox[,(cur_item_family) := item_family == (cur_item_family)]
}

# Make a cat. var. for each store city
store_cities <- unique(sandbox$store_city)
for (i in 1:length(store_cities)) {
  cur_store_city <- store_cities[i]
  sandbox[,(cur_store_city) := store_city == (cur_store_city)]
}
# Make a binary variable for each store cluster
store_clusters <- unique(sandbox$store_cluster)
for (i in 1:length(store_clusters)) {
  cur_store_cluster <- store_clusters[i]
  sandbox[,(paste("Cluster", cur_store_cluster)) := store_cluster == (cur_store_cluster)]
}

# Make a binary variable for each store type
store_types <- unique(sandbox$store_type)
for (i in 1:length(store_types)) {
  cur_store_type <- store_types[i]
  sandbox[,(paste("Store Type", cur_store_type)) := store_type == (cur_store_type)]
}

# Adding months and days since Jan. 1 2013 (both starting at 1)
setDT(sandbox)
sandbox[,month_count := 12 * (year - 2013) + month]
#sandbox[,day_count := 365 * (year - 2013) + day]

```

# 4. Split into transacted/non-transacted (Todo Thomas)
```
# Took off the 'r' tag because we only need to run one time
last_two_weeks = filter(train, as.Date(train$date) >= as.Date("2017-08-1"))
#last_two_weeks = data.table(fread("train2weeks.csv"))
item_ids = unique(last_two_weeks$id)
store_ids = unique(stores$store_nbr)
item_store_sales = data.table(matrix(0, nrow=length(item_ids), ncol=length(store_ids)))
rownames(paste(item_ids))
colnames(paste(store_ids))
for (i in 1:nrow(last_two_weeks)) {
  obs = last_two_weeks[i,]
  item_id = obs$id
  store_id = obs$store_nbr
  item_store_sales[match(item_id, item_ids), match(store_id, store_ids)] = 
    item_store_sales[match(item_id, item_ids), match(store_id, store_ids)] + 1
}

# Need actual matrix of days open for every store
# total_days <- max(last_two_weeks$day_count)
# store_data <- last_two_weeks[,j = .(days_open = (total_days) - min(day_count)), by=store_nbr]
# setkey(store_data, store_nbr)
# days_open <- store_data$days_open

days_open = matrix(15, nrow=1, ncol=length(store_ids))

item_store_sales = data.frame(item_store_sales)

id_frequency = data.frame(item_ids, rep(0,length(item_ids)), rep(0,length(item_ids)))
colnames(id_frequency) = c("id", "store_days", "ratio")
for (i in 1:length(item_ids)) {
  total_transaction_days = sum(item_store_sales[i,])
  days_open_of_stores_w_item = 0
  for (j in 1:length(store_ids)) {
    if (item_store_sales[i,j] > 0) {
      days_open_of_stores_w_item = days_open_of_stores_w_item + days_open[1,store_ids[j]]
    }
  }
  id_frequency[i, 2] = total_transaction_days
  id_frequency[i, 3] = total_transaction_days / days_open_of_stores_w_item
}

fwrite(id_frequency, "id_frequency.csv")

popular_items = filter(id_frequency, store_days==15)
popular_items_ids = popular_items$id


```
# 4b. Splitting sandbox based off popularity
```{r}

# Class var: 0 - RF, 1 - ARIMA, 2 - TEST
setDT(sandbox)
sandbox[,class := ifelse(.I <= (num_rf), 0, ifelse(.I <= (num_rf) + (num_arima), 1, 2))]

# Split sandbox accordingly
setDT(t_pop)
setkey(sandbox, store_nbr, item_nbr)
setkey(t_pop, store_nbr, item_nbr)
s_all <- merge(sandbox, t_pop, all.x = TRUE)

# For NA values, assume popular is false
s_all[,popular := ifelse(is.na(popular),FALSE,popular)]

sandbox_rf2 <- s_all[class == 0]
sandbox_rf2[,class:=NULL]
arima2 <- s_all[class == 1]
arima2[,class:=NULL]
test2 <- s_all[class == 2]
test2[,class:=NULL]

rf_popular <- sandbox_rf2[popular == TRUE]
rf_not_popular <- sandbox_rf2[popular == FALSE]
arima_popular <- arima2[popular == TRUE]
test_popular <- test2[popular == TRUE]
test_not_popular <- test2[popular == FALSE]

```

# 5. Transacted: time series (Justin)

THIS CHUNK: 
1. Set the time series variable
2. Apply ARIMA model on an Example

THIS CHUNK:
1. Generate Data Table of Arima Forecasts by Class
```{r}
library(tidyr)
family_data <- data.frame()
daily_sales <- arima_popular %>% group_by(date,item_family) %>% summarise(daily_class_sales = mean(unit_sales)) %>% filter(date > as.Date('2016-1-1')) #take the data from 2016 onwards 

#checks to see if there are enough obervations aka we need more data points
#we need at least enough data points for each class to have an observation each day
#for (i in unique(daily_sales$item_family)){
#  daily_sales_class <- daily_sales %>% filter(item_family == i)
#  if (nrow(daily_sales_class)<150){
#    daily_sales <- daily_sales %>% filter(item_family != i)
#  }
#}


for (i in unique(daily_sales$item_family)){
  daily_sales_ts <- daily_sales %>% filter(item_family == i) #filter by the class
  ds_ts <- ts(daily_sales_ts$daily_class_sales, frequency = 7) #create the time series with a week period
  ds_ts <- tsclean(ds_ts) #remove outliers
  ds_ts_arima <- auto.arima(ds_ts) #take out stepwise and 
  #approximation to make it go faster
  forecast <- forecast(ds_ts_arima, h= 14) #forecast 14 days ahead
  forecast_values <- forecast$mean #obtain the values for the forecast
  family_data <- rbind(family_data, forecast_values) #add these to the dataframe
}


family_data <- cbind(unique(daily_sales$item_family), family_data) #add family names to data frame
colnames(family_data) <- c("Family",1:14)
family_data
tidy_family_data <- gather(family_data,"Day","Sales",2:15) #tidy up
tidy_family_data$Day <- as.numeric(tidy_family_data$Day)
daily_sales_ts
ggplot() + geom_line(data = tidy_family_data, aes(x= Day, y= Sales, color = Family)) #plot
ggsave("plots/arima_forecasts.jpg")

```



# 6.0 Merge in "popularity" column into test_popular and rf_popular

```{r}
# Clean up to get cols "date", "item_family", "popularity"

train_pop_data <- daily_sales
setDT(train_pop_data)
setnames(train_pop_data, "daily_class_sales", "popularity")

test_pop_data <- tidy_family_data
setDT(test_pop_data)
setnames(test_pop_data, c("Family", "Day", "Sales"), c("item_family","date","popularity"))
test_dates <- unique(test$date)
test_pop_data[,date := (test_dates)[date]]

# Join with rf_popular and test_popular

setkey(rf_popular, date, item_family)
setkey(train_pop_data, date, item_family)
rf_popular <- merge(rf_popular, train_pop_data, all.x = TRUE)

setkey(test_popular, date, item_family)
setkey(test_pop_data, date, item_family)
test_popular <- merge(test_popular, test_pop_data, all.x = TRUE)

# Change NA values to 0
rf_popular[,popularity := ifelse(is.na(popularity),0,popularity)]
test_popular[,popularity := ifelse(is.na(popularity),0,popularity)]

```



# 6. Random forest

```{r}
set.seed(888)

# Train/test data for popular items
s_p_indices <- sample(1:nrow(rf_popular), 0.7*nrow(rf_popular))
s_p_train <- rf_popular[s_p_indices]
s_p_test <- rf_popular[-s_p_indices]

# Train/test data for unpopular items
s_np_indices <- sample(1:nrow(rf_not_popular), 0.7*nrow(rf_not_popular))
s_np_train <- rf_not_popular[s_np_indices]
s_np_test <- rf_not_popular[-s_np_indices]

# Variables to use for the random forest
s_p_cols <- s_np_cols <- c("unit_sales", item_families, paste("Cluster",store_clusters), store_cities, paste("Store Type", store_types), weekday_names, "days_since_payday")
s_p_cols <- c(s_p_cols, "popularity")

# Model for popular items
p_model_data <- s_p_train[,s_p_cols, with=FALSE]
p_model <- train(unit_sales ~ . , data=p_model_data, method="rf",
                  trControl = trainControl(method="oob"),  # Use the OOB error rate 
                  ntree=500, tuneGrid = data.frame(mtry=((length(s_p_cols) - 1))/3),
                  importance = TRUE)  # need this for variable importance below
p_predictions <- predict(p_model, newdata = s_p_test)
p_predictions <- ifelse(p_predictions < 0, 0, p_predictions)
p_actual <- s_p_test$unit_sales
p_actual <- ifelse(p_actual < 0, 0, p_actual)
p_accuracy <- sum((log(p_predictions + 1) - log(p_actual + 1)) ** 2) / length(p_predictions)
p_accuracy

# Model for unpopular items
np_model_data <- s_np_train[,s_np_cols, with=FALSE]
if(nrow(np_model_data) > nrow(p_model_data)) {
  indices <- sample(1:nrow(np_model_data), nrow(p_model_data))
  np_model_data <- np_model_data[indices]
}
np_model <- train(unit_sales ~ . , data=np_model_data, method="rf",
                  trControl = trainControl(method="oob"),  # Use the OOB error rate 
                  ntree=500, tuneGrid = data.frame(mtry=((length(s_np_cols) - 1))/3),
                  importance = TRUE)  # need this for variable importance below
np_predictions <- predict(np_model, newdata = s_np_test)
np_predictions <- ifelse(np_predictions < 0, 0, np_predictions)
np_actual <- s_np_test$unit_sales
np_actual <- ifelse(np_actual < 0, 0, np_actual)
np_accuracy <- sum((log(np_predictions + 1) - log(np_actual + 1)) ** 2) / length(np_predictions)
np_accuracy

# Importance
p_model_imp <- varImp(p_model)$importance
np_model_imp <- varImp(np_model)$importance



# Predict test_popular and test_not_popular
test_p_predictions <- predict(p_model, newdata = test_popular)
test_p_predictions <- ifelse(test_p_predictions < 0, 0, test_p_predictions)
test_np_predictions <- predict(np_model, newdata = test_not_popular)
test_np_predictions <- ifelse(test_np_predictions < 0, 0, test_np_predictions)

```

```{r}

final_predictions <- data.table(id = c(test_popular$id, test_not_popular$id), unit_sales = c(test_p_predictions, test_np_predictions))
fwrite(final_predictions, "final_predictions.csv")

```

# EXTRA STUFF

# A1. Data visualization
```{r}

# How item_family affects sales
item_family_sales <- rf_popular[,j = .(mean_sales = mean(unit_sales)), by = item_family]
ggplot(data = item_family_sales) + geom_col(aes(x = item_family, y = mean_sales)) +
  expand_limits(y = 0) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  xlab("Item Family") + ylab("Mean Unit Sales") + ggtitle("Effect of Item Family for Highly-Transacted Items")
ggsave("plots/1p.jpg")

# How store_city affects sales
store_city_sales <- rf_popular[,j = .(mean_sales = mean(unit_sales)), by = store_city]
ggplot(data = store_city_sales) + geom_col(aes(x = store_city, y = mean_sales)) +
  expand_limits(y = 0) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("Store City") + ylab("Mean Unit Sales") + ggtitle("Effect of Store City for Highly-Transacted Items")
ggsave("plots/2p.jpg")

# How store_cluster affects sales
store_cluster_sales <- rf_popular[,j = .(mean_sales = mean(unit_sales)), by = store_cluster]
ggplot(data = store_cluster_sales) + geom_col(aes(x = store_cluster, y = mean_sales)) +
  expand_limits(y = 0) + 
  xlab("Sore Cluster") + ylab("Mean Unit Sales") + ggtitle("Effect of Store Cluster for Highly-Transacted Items")
ggsave("plots/3p.jpg")

# How store_type affects sales
store_type_sales <- rf_popular[,j = .(mean_sales = mean(unit_sales)), by = store_type]
ggplot(data = store_type_sales) + geom_col(aes(x = store_type, y = mean_sales)) +
  expand_limits(y = 0) + 
  xlab("Store Type") + ylab("Mean Unit Sales") + ggtitle("Effect of Store Type for Highly-Transacted Items")
ggsave("plots/4p.jpg")

# How day of week affects sales
weekday_sales <- rf_popular[,j = .(mean_sales = mean(unit_sales)), by = weekday]
weekday_sales[,weekday := (weekday_names)[weekday + 1]]
weekday_sales$weekday <- factor(weekday_sales$weekday, levels = (weekday_names))
ggplot(data = weekday_sales) + geom_col(aes(x = weekday, y = mean_sales)) +
  expand_limits(y = 0) +
  xlab("Day of Week") + ylab("Mean Unit Sales") + ggtitle("Effect of Day of Week for Highly-Transacted Items")
ggsave("plots/5p.jpg")

# How days since payday affects sales
days_since_payday_sales <- rf_popular[,j = .(mean_sales = mean(unit_sales)), by = days_since_payday]
ggplot(data = days_since_payday_sales) + geom_col(aes(x = days_since_payday, y = mean_sales)) +
  expand_limits(y = 0) + 
  xlab("Days Since Payday") + ylab("Mean Unit Sales") + ggtitle("Effect of Days Since Payday for Highly-Transacted Items")
ggsave("plots/6p.jpg")


## Same code, but for non-highly-transacted items

# How item_family affects sales
item_family_sales <- rf_not_popular[,j = .(mean_sales = mean(unit_sales)), by = item_family]
ggplot(data = item_family_sales) + geom_col(aes(x = item_family, y = mean_sales)) +
  expand_limits(y = 0) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  xlab("Item Family") + ylab("Mean Unit Sales") + ggtitle("Effect of Item Family for Non-Highly-Transacted Items")
ggsave("plots/1np.jpg")

# How store_city affects sales
store_city_sales <- rf_not_popular[,j = .(mean_sales = mean(unit_sales)), by = store_city]
ggplot(data = store_city_sales) + geom_col(aes(x = store_city, y = mean_sales)) +
  expand_limits(y = 0) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("Store City") + ylab("Mean Unit Sales") + ggtitle("Effect of Store City for Non-Highly-Transacted Items")
ggsave("plots/2np.jpg")

# How store_cluster affects sales
store_cluster_sales <- rf_not_popular[,j = .(mean_sales = mean(unit_sales)), by = store_cluster]
ggplot(data = store_cluster_sales) + geom_col(aes(x = store_cluster, y = mean_sales)) +
  expand_limits(y = 0) + 
  xlab("Sore Cluster") + ylab("Mean Unit Sales") + ggtitle("Effect of Store Cluster for Non-Highly-Transacted Items")
ggsave("plots/3np.jpg")

# How store_type affects sales
store_type_sales <- rf_not_popular[,j = .(mean_sales = mean(unit_sales)), by = store_type]
ggplot(data = store_type_sales) + geom_col(aes(x = store_type, y = mean_sales)) +
  expand_limits(y = 0) + 
  xlab("Store Type") + ylab("Mean Unit Sales") + ggtitle("Effect of Store Type for Non-Highly-Transacted Items")
ggsave("plots/4np.jpg")

# How day of week affects sales
weekday_sales <- rf_not_popular[,j = .(mean_sales = mean(unit_sales)), by = weekday]
weekday_sales[,weekday := (weekday_names)[weekday + 1]]
weekday_sales$weekday <- factor(weekday_sales$weekday, levels = (weekday_names))
ggplot(data = weekday_sales) + geom_col(aes(x = weekday, y = mean_sales)) +
  expand_limits(y = 0) +
  xlab("Day of Week") + ylab("Mean Unit Sales") + ggtitle("Effect of Day of Week for Non-Highly-Transacted Items")
ggsave("plots/5np.jpg")

# How days since payday affects sales
days_since_payday_sales <- rf_not_popular[,j = .(mean_sales = mean(unit_sales)), by = days_since_payday]
ggplot(data = days_since_payday_sales) + geom_col(aes(x = days_since_payday, y = mean_sales)) +
  expand_limits(y = 0) + 
  xlab("Days Since Payday") + ylab("Mean Unit Sales") + ggtitle("Effect of Days Since Payday for Non-Highly-Transacted Items")
ggsave("plots/6np.jpg")


```

# A2. Variable importance for two random forests

```{r, fig.width=3,fig.height=7}

ggplot(varImp(p_model, n = 10))
ggsave("plots/varimp_p.jpg")
ggplot(varImp(np_model, n = 10))
ggsave("plots/varimp_np.jpg")

```

# A3. Plotting errors in sample test
```{r}

p_err <- p_predictions - p_actual
p_err_prop <- p_err/p_actual
np_err <- np_predictions - np_actual
np_err_prop <- np_err/np_actual

ggplot(data = data.table(error = p_err)) + geom_density(aes(x = error)) + 
  xlim(-50, 50) + xlab("Prediction Minus Actual") + 
  ggtitle("Random Forest Prediction Errors for Highly-Transacted Items")
ggsave("plots/err_p.jpg")

ggplot(data = data.table(error = p_actual)) + geom_density(aes(x = error)) + 
  xlim(0, 50) + xlab("Unit Sales") + 
  ggtitle("Unit Sales for Highly-Transacted Items")
ggsave("plots/actual_p.jpg")

ggplot(data = data.table(error = p_err_prop)) + geom_density(aes(x = error)) +
  xlim(-2, 15) + xlab("Scaled Error") + 
  ggtitle("Scaled Random Forest Prediction Errors for Highly-Transacted Items")
ggsave("plots/err_scaled_p.jpg")

ggplot(data = data.table(error = np_err)) + geom_density(aes(x = error)) + 
  xlim(-20, 20) + xlab("Prediction Minus Actual") + 
  ggtitle("Random Forest Prediction Errors for Non-Highly-Transacted Items")
ggsave("plots/err_np.jpg")

ggplot(data = data.table(error = np_actual)) + geom_histogram(aes(x = error), binwidth = 1) + 
  xlim(0, 20) + xlab("Unit Sales") + 
  ggtitle("Unit Sales for Non-Highly-Transacted Items")
ggsave("plots/actual_np.jpg")

ggplot(data = data.table(error = np_err_prop)) + geom_density(aes(x = error)) +
  xlim(-2, 15) + xlab("Scaled Error") + 
  ggtitle("Scaled Random Forest Prediction Errors for Non-Highly-Transacted Items")
ggsave("plots/err_scaled_np.jpg")

```

# B. Victor - code for counting popular items

# Make data table for which items are sold every day

tmp <- train[date == "2017-08-01"]
cut <- min(tmp$id)

setkey(train, id)
t_pop <- train[id >= (cut), j = .(n = .N), by = .(store_nbr, item_nbr)]
t_pop[,popular := n == 15]

# Percent sold every day
hist(t_pop$n)
```
